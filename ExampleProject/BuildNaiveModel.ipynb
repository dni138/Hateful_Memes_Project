{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import json\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils import data as data_torch\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ex_path=\"/home/jupyter/Hateful_Memes_Data/babyData/01235.png\"\n",
    "data_ex_path=\"/home/jupyter/Hateful_Memes_Data/babyData/train.jsonl\"\n",
    "bert_path=\"/home/jupyter/ThreeDAnime/bert_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(data_ex_path) as f:\n",
    "    for el in f:\n",
    "        data.append(json.loads(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 42953,\n",
       " 'img': 'img/42953.png',\n",
       " 'label': 0,\n",
       " 'text': 'its their character not their color that matters'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    '''\n",
    "    A really simple network architecture\n",
    "    '''\n",
    "    def __init__(self,input_size, output_size, n_layers,  layer_size, skip_connection=False):\n",
    "        super(SimpleModel,self).__init__()\n",
    "        \n",
    "        self.input_lay=nn.Linear(input_size,layer_size)\n",
    "        self.intermediate_layers:[nn.Sequential]=[]\n",
    "        for n in range(n_layers):\n",
    "            self.intermediate_layers.append(nn.Sequential((nn.Linear(layer_size,layer_size)),nn.ELU()))\n",
    "        \n",
    "        self.output_lay=nn.Linear(layer_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "    \n",
    "        if len(x)!=2:\n",
    "           raise Exception(\"expected batch tuple of image features and text features \")\n",
    "       \n",
    "        x=torch.cat((x[0],x[1]),axis=1)\n",
    "        x=self.input_lay(x)\n",
    "        x=F.elu(x)\n",
    "        for n in self.intermediate_layers:\n",
    "            x=n(x)\n",
    "        x=self.output_lay(x)\n",
    "        x=F.softplus(x,threshold=1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Handle_Simple(data_torch.Dataset):\n",
    "        def __init__(self, imageData, textData,labels):\n",
    "\n",
    "            self.imgData=imageData \n",
    "            self.textData=textData\n",
    "            self.labels=labels\n",
    "\n",
    "        def __len__(self):\n",
    "            'Denotes the total number of samples'\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            'Generates one sample of data'\n",
    "            # Select sample\n",
    "\n",
    "            return self.imgData[index],self.textData[index],self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def train_model_loss(model, epochs,  training_data_handler, learn_rate, trainloader):\n",
    "    train_loss=[]\n",
    "    MAX_EPOCHS=epochs\n",
    "    \n",
    "    if trainloader == None:\n",
    "        trainloader=torch.utils.data.DataLoader(training_data_handler, batch_size=2, shuffle=True,\n",
    "                                                num_workers=0)\n",
    "    \n",
    "    distance=nn.L1Loss(reduction=\"sum\") #I AM BEING SO LAZY  THIS HSOULD BE. CROSSENTROPY, BUT MEHHHHHH\n",
    "    \n",
    "    optimzer=torch.optim.Adam(model.parameters(),lr=learn_rate)\n",
    "\n",
    "    cycle=(iter(trainloader))\n",
    "    temp_train_losses = []\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        cycle=(iter(trainloader))\n",
    "        temp_train_losses_batch = []\n",
    "        for i in range(len(cycle)):\n",
    "            relevant_data=next(cycle)\n",
    "           \n",
    "            input_img=relevant_data[0]\n",
    "            input_txt=relevant_data[1]\n",
    "            ground_truth=relevant_data[2]\n",
    "\n",
    "            output=model((input_img.float(),input_txt.float()))\n",
    "\n",
    "            loss=distance(output,ground_truth)\n",
    "            temp_train_losses_batch.append(loss.detach().numpy())\n",
    "            optimzer.zero_grad()\n",
    "            \n",
    "            if epoch !=0:\n",
    "           \n",
    "\n",
    "                loss.backward()\n",
    "                optimzer.step()\n",
    "         \n",
    "        temp_train_losses.append(np.array(temp_train_losses_batch).mean())\n",
    "\n",
    "    return temp_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a THE ANIMAL THIS IS MOST ASSoICATED with = absolutely need to swap out with mask rcnn\n",
    "def extract_image_features_stupid(data:[], model:torchvision.models.resnet.ResNet, size:(),  max:int=10)->torch.Tensor:\n",
    "    '''\n",
    "    THIS NEEDS TO BE DONE IN BATCHES OR YOU'LL RUN OUT OF MEMORY\n",
    "    Converts. resnet features about. animals for use in our meme model \n",
    "    '''\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.Resize(size),transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    image_transformed=torch.empty(len(data),3,size[0],size[1])\n",
    "    \n",
    "    for image_counter in range(len(data)):  \n",
    "        img_mod=transform(data[image_counter]) \n",
    "        image_transformed[image_counter]=img_mod\n",
    "        image_transformed[image_counter]\n",
    "    \n",
    "    return model(image_transformed).detach()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features_simple(data:[str], model:SentenceTransformer):\n",
    "    return model.encode(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text=SentenceTransformer(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=Image.open(image_ex_path)\n",
    "inception = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS NEEDS TO BE DONE IN BATCHES \n",
    "imageData=extract_image_features_stupid([im,im],inception,(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData=extract_text_features_simple([data[0][\"text\"],data[1][\"text\"]],model_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all=SimpleModel(imageData.shape[1]+textData[0].shape[0],1,1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler=Dataset_Handle_Simple(textData,imageData,torch.tensor([[1],[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=train_model_loss(model_all,30,handler,.01,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8542],\n",
      "        [0.8542]], grad_fn=<SoftplusBackward>)\n"
     ]
    }
   ],
   "source": [
    "results=model_all((imageData,torch.tensor(textData)))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0985398,\n",
       " 1.0985398,\n",
       " 0.13905382,\n",
       " 0.02227515,\n",
       " 0.0040512085,\n",
       " 0.00086164474,\n",
       " 0.00021111965,\n",
       " 5.865097e-05,\n",
       " 1.8239021e-05,\n",
       " 6.198883e-06,\n",
       " 2.3841858e-06,\n",
       " 9.536743e-07,\n",
       " 4.7683716e-07,\n",
       " 2.3841858e-07,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
