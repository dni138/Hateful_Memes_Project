{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hatesonar in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (0.0.7)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from hatesonar) (0.22.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from hatesonar) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from hatesonar) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from hatesonar) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from hatesonar) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.22.0->hatesonar) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.22.0->hatesonar) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->hatesonar) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hatesonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/forrest.xiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from LoadingData import LoadingData\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from hatesonar import Sonar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_getty_data():\n",
    "    data = LoadingData.LoadingData(\"~/projects/Hateful_Memes_Project/data/getty_data_full.csv\")\n",
    "    getty=data.load_data()\n",
    "    df=getty[~getty['image'].str.contains(\"\\(\")]\n",
    "    getty[~getty['image'].str.contains(\"\\(\")].image.nunique()\n",
    "    \n",
    "    df['color_zscore'] = (df['Color_Score'] - df['Color_Score'].mean())/df['Color_Score'].std(ddof=0)\n",
    "    df['key_point_zscore'] = (df['Key_Point_Score'] - df['Key_Point_Score'].mean())/df['Key_Point_Score'].std(ddof=0)\n",
    "    df['combined_z']=df['color_zscore']+df['key_point_zscore']\n",
    "    df['combined_zscore']=(df['combined_z'] - df['combined_z'].mean())/df['combined_z'].std(ddof=0)\n",
    "    \n",
    "#     s = df.combined_zscore\n",
    "#     ax = s.plot.kde()\n",
    "#     display(df.combined_zscore.describe())\n",
    "    \n",
    "    combined_index = df.groupby('image')['combined_zscore'].idxmin()\n",
    "    match=df.loc[combined_index]\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_dataset():\n",
    "    df_list = []\n",
    "\n",
    "    for x in ['train','dev','test']:\n",
    "        file=\"../data/\"+x+\".jsonl\"\n",
    "        data=LoadingData.LoadingData(file)\n",
    "        data_json=data.load_data()\n",
    "        df=pd.DataFrame(data_json)\n",
    "        df['partition']=x\n",
    "        if x=='test':\n",
    "            df['label']=np.nan\n",
    "        df_list.append(df)\n",
    "    \n",
    "    final = pd.concat(df_list)\n",
    "    return final\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meme_text_hate_speech_features(clean):\n",
    "    sonar = Sonar()\n",
    "    clean['hate_speech']=clean.text.apply(lambda x: sonar.ping(text=str(x or '')).get('classes')[0].get('confidence'))\n",
    "    clean['offensive_langauge']=clean.text.apply(lambda x: sonar.ping(text=str(x or '')).get('classes')[1].get('confidence'))\n",
    "    clean['neither']=clean.text.apply(lambda x: sonar.ping(text=str(x or '')).get('classes')[2].get('confidence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_getty_features():\n",
    "    #load data.  Match is best matching Getty data and df is full dataframe of train/test/dev data\n",
    "    match=clean_getty_data()\n",
    "    df=load_full_dataset()\n",
    "    \n",
    "    \n",
    "    #clean up types, column names, and text\n",
    "    match['src']=match['src'].astype(str)\n",
    "    df['id']=df['id'].astype(str)\n",
    "    match.rename(columns={'id':'rank'}, inplace=True)\n",
    "    \n",
    "    full=df.merge(match, how='left', left_on='id', right_on='src')\n",
    "    full['tags_clean']= full['tags'].str.replace(',',' ').str.replace('Photos',' ').str.lower().str.split()\n",
    "    \n",
    "    #tokenize text\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    full['text_clean'] = full['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    \n",
    "    clean=full[['id','img','text','text_clean','partition','tags','tags_clean','caption','combined_zscore','label']]\n",
    "    \n",
    "    #calculate text frequencies\n",
    "    clean['all_text_freq']= clean.text.map(clean.text.value_counts())\n",
    "    clean['train_text_freq']= clean.text.map(clean[clean.partition=='train'].text.value_counts())\n",
    "    clean['dev_text_freq']= clean.text.map(clean[clean.partition=='dev'].text.value_counts())\n",
    "    clean['test_text_freq']= clean.text.map(clean[clean.partition=='test'].text.value_counts())\n",
    "\n",
    "    # manual lists for features.  This snowballed into something ugly, refactor in a better way, maybe dict.\n",
    "    race=['african','black','asian','chinese','arab','white','african-american','ethnicity']\n",
    "    disability=['disabled','disability',\"down's, syndrome\",'retarded','retarted']\n",
    "    religion=['islam','muslim','muslims','catholic','catholics','christian','christians','jewish','jew','jews',\n",
    "             'god','jesus']\n",
    "    sexual_orientation=['gay','straight','trans','transexual','homo','homosexual']\n",
    "    violence=['obscene','anger','aggression','kill','killing','bomb','gun']\n",
    "\n",
    "    criminals=['hitler']\n",
    "    animals=['alligator','ant','bear','bee','bird','camel','cat','cheetah','chicken','chimpanzee','cow','crocodile','deer','dog','dolphin','duck','eagle','elephant','fish','fly','fox','frog','giraffe','goat','goldfish','hamster','hippopotamus','horse','kangaroo','kitten','lion','lobster','monkey','octopus','owl','panda','pig','puppy','rabbit','rat','scorpion','seal','shark','sheep','snail','snake','spider','squirrel','tiger','turtle','wolf','zebra']\n",
    "\n",
    "    gender=['man','woman','men','women','bitch','pussy']\n",
    "\n",
    "    #simple features for baseline\n",
    "    clean['tags_clean'] = [ [] if x is np.NaN else x for x in clean['tags_clean'] ]\n",
    "    clean['tags_race'] = clean.apply(lambda row: True if any(item in race for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_disability'] = clean.apply(lambda row: True if any(item in disability for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_religion'] = clean.apply(lambda row: True if any(item in religion for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_sexual_orientation'] = clean.apply(lambda row: True if any(item in sexual_orientation for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_violence'] = clean.apply(lambda row: True if any(item in violence for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_criminals'] = clean.apply(lambda row: True if any(item in criminals for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_gender'] = clean.apply(lambda row: True if any(item in gender for item in row['tags_clean']) else False, axis = 1)\n",
    "    clean['tags_animals'] = clean.apply(lambda row: True if any(item in animals for item in row['tags_clean']) else False, axis = 1)\n",
    "\n",
    "    clean['text_clean'] = [ [] if x is np.NaN else x for x in clean['text_clean'] ]\n",
    "    clean['text_race'] = clean.apply(lambda row: True if any(item in race for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_disability'] = clean.apply(lambda row: True if any(item in disability for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_religion'] = clean.apply(lambda row: True if any(item in religion for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_sexual_orientation'] = clean.apply(lambda row: True if any(item in sexual_orientation for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_violence'] = clean.apply(lambda row: True if any(item in violence for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_criminals'] = clean.apply(lambda row: True if any(item in criminals for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_gender'] = clean.apply(lambda row: True if any(item in gender for item in row['text_clean']) else False, axis = 1)\n",
    "    clean['text_animals'] = clean.apply(lambda row: True if any(item in animals for item in row['text_clean']) else False, axis = 1)\n",
    "    \n",
    "    #Add hate speech features\n",
    "    create_meme_text_hate_speech_features(clean)\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_getty_features(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/forrest.xiao/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ddb5470dc83c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_simple_getty_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwrite_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../data/getty_clean_simple_features.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-6c1942da217d>\u001b[0m in \u001b[0;36mwrite_data\u001b[0;34m(data, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "clean=create_simple_getty_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.to_csv(\"../data/getty_clean_simple_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
