{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentence_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing_data=pd.read_csv(\"../data/bing.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "getty_data_pd=pd.read_csv(\"../data/cleaned_getty_data.csv\",index_col=0)\n",
    "\n",
    "getty_data_pd = getty_data_pd.replace(np.nan, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "getty_data_pd=getty_data_pd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_encoder=sentence_encoder.SentenceTransform(\"roberta-base-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_encoder=sentence_encoder.SentenceTransform(\"average_word_embeddings_glove.6B.300d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_caption_embedding=mega.view(6650,768).mean(axis=1)\n",
    "mean_caption_embedding_numpy=[mean_caption_embedding.detach().numpy()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean for full sentences \n",
    "\n",
    "def embed_mean(word_embedding_model,text_column:list,max_number:int):\n",
    "    counter=0\n",
    "    all_captions=[]\n",
    "    for caption in text_column:\n",
    "        if counter>max_number:\n",
    "            break \n",
    "        if len(caption)==0:\n",
    "            continue\n",
    "        else:\n",
    "            result=big_encoder.extract_text_features_simple([caption])\n",
    "            all_captions.append(result)\n",
    "        counter+=1\n",
    "\n",
    "    mega=torch.Tensor(all_captions)\n",
    "    return mega.view(mega.shape[0],mega.shape[2]).mean(dim=0)\n",
    "\n",
    "\n",
    "def embed_column(word_embedding_model, text_column:list, mean_embedd, max_number:int):\n",
    "    \n",
    "    counter=0\n",
    "    all_captions=[]\n",
    "    for caption in getty_data_pd[\"caption\"].values:\n",
    "        if counter>max_number:\n",
    "            break \n",
    "        if len(caption)==0:\n",
    "            all_captions.append([mean_embedd])\n",
    "\n",
    "        else:\n",
    "            result=big_encoder.extract_text_features_simple([caption])\n",
    "            all_captions.append(result)\n",
    "        counter+=1\n",
    "    return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding=embed_mean(big_encoder,getty_data_pd[\"caption\"].values,50)\n",
    "all_data=embed_column(big_encoder,getty_data_pd[\"caption\"].values,mean_embedding,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Opioid   Syringe   Addiction   Despair   Heroin   Addict   Drug Overdose   Recreational Drug   Teenager   IV Drip   Abuse   Adult Amphetamine   Close-Up   Cocaine   Criminal   Danger   Dependency   Depression - Sadness   Emotional Stress   Forbidden   Frustration   Grief   Hand   Healthcare And Medicine   Horizontal   Human Body Part   Human Hand   Illness   Photography   Problems   Risk   Sharp   Snorting   Thailand   Vein   Worried   Photos'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getty_data_pd[\"best_tags\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=glove_encoder.extract_text_features_simple(getty_data_pd[\"best_tags\"][0].replace(\"  \",\" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_glove_tags(glove_embedding_model,tag_column_list,mean_embed, max_number):\n",
    "    final_data=[]\n",
    "    for i in range(max_number):\n",
    "        tags_to_look_at=tag_column_list[i]\n",
    "       \n",
    "        \n",
    "        result=_select_tags_glove(glove_embedding_model,tags_to_look_at)\n",
    "  \n",
    "        result=torch.Tensor(result).mean(dim=0).detach().numpy()\n",
    "        final_data.append(result)\n",
    "        \n",
    "    return final_data\n",
    "        \n",
    "def _select_tags_glove(glove_embedding_model,tag_string:str):\n",
    "    #just take out the tags that are actual words \n",
    "    tags=tag_string.replace(\"  \",\" \").split(\" \")\n",
    "\n",
    "    sub_tags=[]\n",
    "    for tag in tags: \n",
    "        if(tag!=\"\"):\n",
    "            sub_tags.append(tag)\n",
    "    \n",
    "    \n",
    "    temp_result=glove_embedding_model.extract_text_features_simple(sub_tags)\n",
    "    final_final=[]\n",
    "\n",
    "    for temp in temp_result:\n",
    "        if temp[0]!=0:\n",
    "  \n",
    "            final_final.append(temp)\n",
    " \n",
    "    return final_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=embed_glove_tags(glove_encoder,getty_data_pd[\"best_tags\"].values,None,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 300])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoder.GettySentenceTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls1=[\"Ethnicity\",\"Religion\",\"Sexual Orientation\",\"Gender\",\"Gender Identity\",\"Disability Disease\",\"Nationality\",\n",
    "      \"Immigration Status\"]\n",
    "ex1=[\"African\",\"Muslim\",\"Lesbian\",\"Male\",\"Transgender\",\"Autistic\",\"Nationality\",\"Migrant\",\"Poor\"]\n",
    "ex2=[\"White\",\"Christian\",\"Gay\",\"Woman\",\"Queer\",\"Cancer\",\"Nationality\",\"Undocumented\",\"Rich\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtectedClassifierSimple():\n",
    "    \n",
    "    def __init__(self, sentence_encoder_model:sentence_encoder.SentenceTransform):\n",
    "        cls1=[\"Ethnicity\",\"Religion\",\"Sexual Orientation\",\"Gender\",\"Gender Identity\",\"Disability Disease\",\"Nationality\",\n",
    "      \"Immigration Status\",\"socioeconomic Status\"]\n",
    "        ex1=[\"african\",\"muslim\",\"lesbian\",\"male\",\"transgender\",\"autistic\",\"nationality\",\"migrant\",\"poor\"]\n",
    "        ex2=[\"white\",\"jewish\",\"gay\",\"woman\",\"queer\",\"cancer\",\"nationality\",\"undocumented\",\"rich\"]\n",
    "        \n",
    "        self.class_names=cls1\n",
    "        \n",
    "        self.transformer=sentence_encoder_model\n",
    "        \n",
    "        self.comparison_names=[]\n",
    "        self.comparison_names.append(ex1)\n",
    "        self.comparison_names.append(ex2)\n",
    "        \n",
    "        self.featurized_cl1=sentence_encoder_model.extract_text_features_simple(cls1)\n",
    "        #self.featurized_ex1=sentence_encoder_model.extract_text_features_simple(ex1)\n",
    "        #self.featurized_ex2=sentence_encoder_model.extract_text_features_simple(ex2)\n",
    "        \n",
    "    def measure_distance(self, sentences:list):\n",
    "        \n",
    "        transformed_sentences:list=self.transformer.extract_text_features_simple(sentences)\n",
    "            \n",
    "        #first= manhattan_distances(transformed_sentences,self.featurized_ex1)\n",
    "        #second=manhattan_distances(transformed_sentences,self.featurized_ex2)\n",
    "        third=manhattan_distances(transformed_sentences,self.featurized_cl1)\n",
    "        \n",
    "        return third #, np.var((first,second,third),axis=0)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "t=ProtectedClassifierSimple(big_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "t.measure_distance([\"kill me\",\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=t.measure_distance([\"nigger\",\"jew\",\"retard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[482.59958776, 498.66377375, 599.21235833, 549.47485269,\n",
       "        564.35719589, 527.3622152 , 482.12608542, 563.26669934,\n",
       "        535.74760391],\n",
       "       [503.92217677, 496.59318416, 542.47142668, 544.35116678,\n",
       "        561.4483124 , 532.68924958, 550.86909706, 552.19209644,\n",
       "        535.16344658],\n",
       "       [604.7842411 , 534.75539984, 610.30460624, 567.01158396,\n",
       "        603.22137152, 459.59280359, 606.25180061, 570.24727427,\n",
       "        543.93922696]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.21925454e+03, 2.26203724e+03, 2.33195627e+03, 1.67984608e+02,\n",
       "        1.28687521e+03, 1.28840382e+03, 1.15438061e-09, 2.35005113e+03,\n",
       "        7.55686391e+02],\n",
       "       [1.15023501e+02, 3.29921196e+04, 7.97214987e+02, 3.19175431e+02,\n",
       "        6.11152723e+02, 9.85408460e+01, 9.35449067e-10, 3.44904628e+02,\n",
       "        2.76897586e+02],\n",
       "       [5.39704342e+02, 9.29523890e+02, 8.15851335e+02, 6.91416671e+02,\n",
       "        2.29674230e+02, 4.01866856e+03, 1.39534305e-09, 3.16289666e+02,\n",
       "        1.59720702e+03]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
