{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30-08-2020:16:40:00,781 INFO     [file_utils.py:39] PyTorch version 1.4.0 available.\n",
      "30-08-2020:16:40:00,782 INFO     [file_utils.py:55] TensorFlow version 2.2.0 available.\n",
      "[nltk_data] Downloading package punkt to /Users/nissani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/nissani/Desktop/Hateful_Memes_Project/LoadingData')\n",
    "sys.path.append('/Users/nissani/Desktop/Hateful_Memes_Project/FeatureGeneration')\n",
    "import os\n",
    "import LoadingData\n",
    "from FER_featurizer import FER_Wrapper\n",
    "from hate_speech import HateWrapper\n",
    "from sentence_encoder import SentenceTransformer\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import torch\n",
    "import torchvision\n",
    "import sentence_encoder\n",
    "import protected_classifier_naive\n",
    "import getty_simple_featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = LoadingData.LoadingData('/Users/nissani/Desktop/Hateful_Memes_Project/data/train.jsonl')\n",
    "dev_data = LoadingData.LoadingData('/Users/nissani/Desktop/Hateful_Memes_Project/data/dev.jsonl')\n",
    "image_features = LoadingData.LoadingData('/Users/nissani/Desktop/Hateful_Memes_Project/data/cleaned_getty_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = dev_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_features.load_data()\n",
    "image_features = image_features.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9983"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Getty Data\n",
    "\n",
    "In order for the sentiment analyzer to work correctly, when we do not have any text, we put in \"okay\" as a neutral word for a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_features.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_captions = {}\n",
    "best_tags = {}\n",
    "for best_caption, best_tag, src in zip(list(image_features.best_caption), list(image_features.best_tags), list(image_features.src)):\n",
    "    if ((not best_caption) and (not best_tag)):\n",
    "        best_captions[src] = 'okay'\n",
    "        best_tags[src] = 'okay'\n",
    "    elif not best_caption:\n",
    "        best_captions[src] = best_tag\n",
    "        best_tags[src] = best_tag\n",
    "    elif not best_tag:\n",
    "        best_captions[src] = best_caption\n",
    "        best_tags[src] = best_caption\n",
    "    else:\n",
    "        best_captions[src] = best_caption\n",
    "        best_tags[src] = best_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_captions_tags(data, tags, captions):\n",
    "    for el in data:\n",
    "        identifier = el['id']\n",
    "        try:\n",
    "            el['tag'] = tags[identifier]\n",
    "        except:\n",
    "            el['tag'] = 'no tag'\n",
    "        try:\n",
    "            el['caption'] = captions[identifier]\n",
    "        except:\n",
    "            el['caption'] = 'no caption'\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = add_captions_tags(train_data, best_tags, best_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "\n",
    "This function does multiple things:\n",
    "\n",
    "1) It instantiates multiple objects that featurize the data differently. FER creates emotion features when available. Hate creates hate and offensive scores. These are the first lines of code, and new objects should be instantiated in the same place.\n",
    "\n",
    "2) The next major part is the four loop. Here we create all the features for each component of text. Note, that we have the captions, the tags, and the meme text. Each component should be separate (as opposed to the baseline), and we should make sure to keep them separate in each feature generation step.\n",
    "\n",
    "3) The last part of this function makes the entire list of dictionaries into a dictionary of dictionaries, where the key is the picture id and the dictionary contains all the information and features we could want about the images.\n",
    "\n",
    "Inputs: data (json lines), captions (array of strings), tags (array of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    FER = FER_Wrapper()\n",
    "    hate = HateWrapper()\n",
    "    sentiment_classifier = TextClassifier.load('en-sentiment')\n",
    "    glove_encoder = sentence_encoder.GettySentenceTransform(\"average_word_embeddings_glove.6B.300d\")\n",
    "    bert_encoder = sentence_encoder.GettySentenceTransform(\"roberta-base-nli-stsb-mean-tokens\")\n",
    "    protected_classifier = protected_classifier_naive.ProtectedClassifierSimple(bert_encoder)\n",
    "    getty_features = getty_simple_featurizer.GettySimpleWrapper()\n",
    "    \n",
    "    aggregate_text = []\n",
    "    aggregate_captions = []\n",
    "    aggregate_tags = []\n",
    "    for el in data:\n",
    "        aggregate_text.append(el['text'])\n",
    "        aggregate_tags.append(el['tag'])\n",
    "        aggregate_captions.append(el['caption'])\n",
    "    \n",
    "    #hate scores\n",
    "    meme_hate_scores = hate.predict(aggregate_text)\n",
    "    caption_hate_scores = hate.predict(captions)\n",
    "    tag_hate_scores = hate.predict(tags)\n",
    "    \n",
    "    #word embeddings\n",
    "    tag_feature_vectors = glove_encoder.embed_glove_tags(glove_encoder, tags, None, len(tags))\n",
    "    bert_feature_vectors = bert_encoder.embed_column(bert_encoder, captions, None, len(captions))\n",
    "    bert_feature_vectors = torch.Tensor(bert_feature_vectors).view((len(captions),768))\n",
    "    meme_feature_vectors = bert_encoder.embed_column(bert_encoder, aggregate_text, None, len(aggregate_text))\n",
    "    meme_feature_vectors = torch.Tensor(meme_feature_vectors).view((len(aggregate_text),768))\n",
    "    \n",
    "    #protected classifier\n",
    "    protected_memes = protected_classifier.measure_distance(aggregate_text)\n",
    "    protected_captions = protected_classifier.measure_distance(captions)\n",
    "    \n",
    "    #getty features\n",
    "    getty = getty_features.get_simple_getty_features()\n",
    "    getty = getty.fillna(0)\n",
    "    \n",
    "    for meme, caption, tag, tag_vector, bert_vector, meme_vector, meme_scores, caption_scores in zip(data, \n",
    "                                                                                                    captions, \n",
    "                                                                                                    tags, \n",
    "                                                                                                    tag_feature_vectors, \n",
    "                                                                                                    bert_feature_vectors,\n",
    "                                                                                                    meme_feature_vectors,\n",
    "                                                                                                    protected_memes,\n",
    "                                                                                                    protected_captions):\n",
    "        \n",
    "        meme['meme_hate_speech'] = meme_hate_scores[meme_hate_scores.text == meme['text']].hate_speech\n",
    "        meme['meme_offensive_language'] = meme_hate_scores[meme_hate_scores.text == meme['text']].offensive_language\n",
    "        meme['meme_neither'] = meme_hate_scores[meme_hate_scores.text == meme['text']].neither\n",
    "        meme['caption_hate_speech'] = caption_hate_scores[caption_hate_scores.text == caption].hate_speech\n",
    "        meme['caption_offensive_language'] = caption_hate_scores[caption_hate_scores.text == caption].offensive_language\n",
    "        meme['caption_neither'] = caption_hate_scores[caption_hate_scores.text == caption].neither\n",
    "        meme['tag_hate_speech'] = tag_hate_scores[tag_hate_scores.text == tag].hate_speech\n",
    "        meme['tag_offensive_language'] = tag_hate_scores[tag_hate_scores.text == tag].offensive_language\n",
    "        meme['tag_neither'] = tag_hate_scores[tag_hate_scores.text == tag].neither\n",
    "    \n",
    "        sentence = Sentence(meme['text'])\n",
    "        sentiment_classifier.predict(sentence)\n",
    "        label = str(sentence.labels[0]).split(' ')[0]\n",
    "        proba = float(str(sentence.labels[0]).split(' ')[1].replace('(', '').replace(')', ''))\n",
    "        meme['meme_sentiment'] = [label, proba]\n",
    "        \n",
    "        sentence = Sentence(caption)\n",
    "        sentiment_classifier.predict(sentence)\n",
    "        label = str(sentence.labels[0]).split(' ')[0]\n",
    "        proba = float(str(sentence.labels[0]).split(' ')[1].replace('(', '').replace(')', ''))\n",
    "        meme['caption_sentiment'] = [label, proba]\n",
    "        \n",
    "        sentence = Sentence(tag)\n",
    "        sentiment_classifier.predict(sentence)\n",
    "        label = str(sentence.labels[0]).split(' ')[0]\n",
    "        proba = float(str(sentence.labels[0]).split(' ')[1].replace('(', '').replace(')', ''))\n",
    "        meme['tag_sentiment'] = [label, proba]\n",
    "        \n",
    "        path = '/Users/nissani/Desktop/Hateful_Memes_Project/data/' + meme['img']\n",
    "        emotion_feature = FER.run_FER(path)\n",
    "        img_id = meme['img'].split('/')[1]\n",
    "        meme['emotion_feature'] = emotion_feature[img_id]\n",
    "        \n",
    "        meme['tag_feature_vector'] = tag_vector\n",
    "        meme['caption_feature_vector'] = bert_vector\n",
    "        \n",
    "        meme['protected_meme_scores'] = meme_scores\n",
    "        meme['protected_caption_scores'] = caption_scores\n",
    "        \n",
    "    new_data = {item['id'] : item for item in data}\n",
    "    getty_columns = list(getty.columns)\n",
    "    \n",
    "    for el in list(new_data.keys()):\n",
    "        getty_point = gett[getty.id == str(el)]\n",
    "        for name in getty_columns:\n",
    "            new_data[el][name] = getty_point['name'].values[0]\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-30 16:22:59,569 loading file /Users/nissani/.flair/models/sentiment-en-mix-distillbert.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "30-08-2020:16:23:00,346 INFO     [configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /Users/nissani/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "30-08-2020:16:23:00,347 INFO     [configuration_utils.py:301] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "30-08-2020:16:23:00,492 INFO     [tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/nissani/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "30-08-2020:16:23:11,388 INFO     [SentenceTransformer.py:29] Load pretrained SentenceTransformer: average_word_embeddings_glove.6B.300d\n",
      "30-08-2020:16:23:11,389 INFO     [SentenceTransformer.py:32] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "30-08-2020:16:23:11,401 INFO     [SentenceTransformer.py:67] Load SentenceTransformer from folder: /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_average_word_embeddings_glove.6B.300d.zip\n",
      "30-08-2020:16:23:14,476 INFO     [SentenceTransformer.py:88] Use pytorch device: cpu\n",
      "30-08-2020:16:23:14,477 INFO     [SentenceTransformer.py:29] Load pretrained SentenceTransformer: roberta-base-nli-stsb-mean-tokens\n",
      "30-08-2020:16:23:14,478 INFO     [SentenceTransformer.py:32] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "30-08-2020:16:23:14,480 INFO     [SentenceTransformer.py:67] Load SentenceTransformer from folder: /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip\n",
      "30-08-2020:16:23:14,495 INFO     [configuration_utils.py:263] loading configuration file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/config.json\n",
      "30-08-2020:16:23:14,496 INFO     [configuration_utils.py:301] Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "30-08-2020:16:23:14,500 INFO     [modeling_utils.py:648] loading weights file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/pytorch_model.bin\n",
      "30-08-2020:16:23:19,1 INFO     [tokenization_utils.py:938] Model name '/Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "30-08-2020:16:23:19,3 INFO     [tokenization_utils.py:1020] loading file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/vocab.json\n",
      "30-08-2020:16:23:19,44 INFO     [tokenization_utils.py:1020] loading file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/merges.txt\n",
      "30-08-2020:16:23:19,45 INFO     [tokenization_utils.py:1020] loading file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/added_tokens.json\n",
      "30-08-2020:16:23:19,46 INFO     [tokenization_utils.py:1020] loading file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/special_tokens_map.json\n",
      "30-08-2020:16:23:19,70 INFO     [tokenization_utils.py:1020] loading file /Users/nissani/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_roberta-base-nli-stsb-mean-tokens.zip/0_RoBERTa/tokenizer_config.json\n",
      "30-08-2020:16:23:20,444 INFO     [SentenceTransformer.py:88] Use pytorch device: cpu\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 251.22it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 586.09it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 179.32it/s]\n",
      "Batches: 100%|██████████| 11/11 [00:00<00:00, 744.25it/s]\n",
      "Batches: 100%|██████████| 11/11 [00:00<00:00, 1385.17it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 979.45it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 1459.40it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 543.70it/s]\n",
      "Batches: 100%|██████████| 10/10 [00:00<00:00, 763.02it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 478.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.95it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n"
     ]
    }
   ],
   "source": [
    "featurized_train_data = create_features(train_data[:10], best_captions[:10], best_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
