{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/nissani/Desktop/Hateful_Memes_Project/LoadingData')\n",
    "sys.path.append('/Users/nissani/Desktop/Hateful_Memes_Project/FeatureGeneration')\n",
    "import os\n",
    "import LoadingData\n",
    "from FER_featurizer import FER_Wrapper\n",
    "from hate_speech import HateWrapper\n",
    "from sentence_encoder import SentenceTransformer\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = LoadingData.LoadingData('/Users/nissani/Desktop/Hateful_Memes_Project/data/train.jsonl')\n",
    "dev_data = LoadingData.LoadingData('/Users/nissani/Desktop/Hateful_Memes_Project/data/dev.jsonl')\n",
    "image_features = LoadingData.LoadingData('/Users/nissani/Desktop/Hateful_Memes_Project/data/cleaned_getty_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = dev_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_features.load_data()\n",
    "image_features = image_features.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Getty Data\n",
    "\n",
    "In order for the sentiment analyzer to work correctly, when we do not have any text, we put in \"okay\" as a neutral word for a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_features.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_captions = []\n",
    "best_tags = []\n",
    "for best_caption, best_tag, src in zip(list(image_features.best_caption), list(image_features.best_tags), list(image_features.src)):\n",
    "    if ((not best_caption) and (not best_tag)):\n",
    "        best_captions.append('okay')\n",
    "        best_tags.append('okay')\n",
    "    elif not best_caption:\n",
    "        best_captions.append(best_tag)\n",
    "        best_tags.append(best_tag)\n",
    "    elif not best_tag:\n",
    "        best_captions.append(best_caption)\n",
    "        best_tags.append(best_caption)\n",
    "    else:\n",
    "        best_captions.append(best_caption)\n",
    "        best_tags.append(best_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "\n",
    "This function does multiple things:\n",
    "\n",
    "1) It instantiates multiple objects that featurize the data differently. FER creates emotion features when available. Hate creates hate and offensive scores. These are the first lines of code, and new objects should be instantiated in the same place.\n",
    "\n",
    "2) The next major part is the four loop. Here we create all the features for each component of text. Note, that we have the captions, the tags, and the meme text. Each component should be separate (as opposed to the baseline), and we should make sure to keep them separate in each feature generation step.\n",
    "\n",
    "3) The last part of this function makes the entire list of dictionaries into a dictionary of dictionaries, where the key is the picture id and the dictionary contains all the information and features we could want about the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data, captions, tags):\n",
    "    FER = FER_Wrapper()\n",
    "    hate = HateWrapper()\n",
    "    sentiment_classifier = TextClassifier.load('en-sentiment')\n",
    "    \n",
    "    aggregate_text = []\n",
    "    for el in data:\n",
    "        aggregate_text.append(el['text'])\n",
    "\n",
    "    meme_hate_scores = hate.predict(aggregate_text)\n",
    "    caption_hate_scores = hate.predict(captions)\n",
    "    tag_hate_scores = hate.predict(tags)\n",
    "    for meme, caption, tag in zip(data, captions, tags):\n",
    "        meme['meme_hate_speech'] = meme_hate_scores[meme_hate_scores.text == meme['text']].hate_speech\n",
    "        meme['meme_offensive_language'] = meme_hate_scores[meme_hate_scores.text == meme['text']].offensive_language\n",
    "        meme['meme_neither'] = meme_hate_scores[meme_hate_scores.text == meme['text']].neither\n",
    "        meme['caption_hate_speech'] = caption_hate_scores[caption_hate_scores.text == caption].hate_speech\n",
    "        meme['caption_offensive_language'] = caption_hate_scores[caption_hate_scores.text == caption].offensive_language\n",
    "        meme['caption_neither'] = caption_hate_scores[caption_hate_scores.text == caption].neither\n",
    "        meme['tag_hate_speech'] = tag_hate_scores[tag_hate_scores.text == tag].hate_speech\n",
    "        meme['tag_offensive_language'] = tag_hate_scores[tag_hate_scores.text == tag].offensive_language\n",
    "        meme['tag_neither'] = tag_hate_scores[tag_hate_scores.text == tag].neither\n",
    "    \n",
    "        sentence = Sentence(meme['text'])\n",
    "        sentiment_classifier.predict(sentence)\n",
    "        label = str(sentence.labels[0]).split(' ')[0]\n",
    "        proba = float(str(sentence.labels[0]).split(' ')[1].replace('(', '').replace(')', ''))\n",
    "        meme['meme_sentiment'] = [label, proba]\n",
    "        \n",
    "        sentence = Sentence(caption)\n",
    "        sentiment_classifier.predict(sentence)\n",
    "        label = str(sentence.labels[0]).split(' ')[0]\n",
    "        proba = float(str(sentence.labels[0]).split(' ')[1].replace('(', '').replace(')', ''))\n",
    "        meme['caption_sentiment'] = [label, proba]\n",
    "        \n",
    "        sentence = Sentence(tag)\n",
    "        sentiment_classifier.predict(sentence)\n",
    "        label = str(sentence.labels[0]).split(' ')[0]\n",
    "        proba = float(str(sentence.labels[0]).split(' ')[1].replace('(', '').replace(')', ''))\n",
    "        meme['tag_sentiment'] = [label, proba]\n",
    "        \n",
    "        path = '/Users/nissani/Desktop/Hateful_Memes_Project/data/' + meme['img']\n",
    "        emotion_feature = FER.run_FER(path)\n",
    "        img_id = meme['img'].split('/')[1]\n",
    "        meme['emotion_feature'] = emotion_feature[img_id]\n",
    "        \n",
    "    new_data = {item['id'] : item for item in data}\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-30 15:06:11,154 loading file /Users/nissani/.flair/models/sentiment-en-mix-distillbert.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "30-08-2020:15:06:11,392 INFO     [configuration_utils.py:265] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /Users/nissani/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "30-08-2020:15:06:11,393 INFO     [configuration_utils.py:301] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "30-08-2020:15:06:11,578 INFO     [tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/nissani/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n"
     ]
    }
   ],
   "source": [
    "featurized_train_data = create_features(train_data, best_captions, best_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
